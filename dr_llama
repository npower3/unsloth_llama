#!/usr/bin/env python3
"""
Fine-tuning script for LLaMA using Unsloth, with repetition fix using:
- [INST] ... [/INST] format
- Custom collator for masked label training
- Improved generation settings
"""

# Core imports
import torch
from unsloth import FastLanguageModel, is_bfloat16_supported
from datasets import load_dataset
from trl import SFTTrainer, SFTConfig
from transformers import TrainerCallback
from dataclasses import dataclass
import json

# Configuration
MAX_SEQ_LENGTH = 2048
DTYPE = None
LOAD_IN_4BIT = False
MODEL_LOC = "your_model_path_or_hf_repo"

# Custom data collator with prompt masking
@dataclass
class PromptCompletionCollator:
    tokenizer: any
    max_length: int = 2048

    def __call__(self, examples):
        input_ids_list, labels_list = [], []
        for ex in examples:
            user = ex.get("instruction") or ex.get("prompt") or ex.get("input")
            assistant = ex.get("response") or ex.get("output") or ex.get("answer")
            prompt = f"<s>[INST] {user.strip()} [/INST] "
            completion = f"{assistant.strip()}</s>"
            full_text = prompt + completion

            ids = self.tokenizer(full_text, truncation=True, max_length=self.max_length, add_special_tokens=False)["input_ids"]
            labels = ids.copy()
            mask_len = len(self.tokenizer(prompt, add_special_tokens=False)["input_ids"])
            labels[:mask_len] = [-100] * min(mask_len, len(labels))
            input_ids_list.append(torch.tensor(ids))
            labels_list.append(torch.tensor(labels))

        input_ids_batch = torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)
        labels_batch = torch.nn.utils.rnn.pad_sequence(labels_list, batch_first=True, padding_value=-100)
        attention_mask = (input_ids_batch != self.tokenizer.pad_token_id).long()

        return {"input_ids": input_ids_batch, "attention_mask": attention_mask, "labels": labels_batch}

# Load model and tokenizer
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_LOC,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=DTYPE,
    load_in_4bit=LOAD_IN_4BIT,
)

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({"pad_token": "<pad>"})
    model.resize_token_embeddings(len(tokenizer))

model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
    use_rslora=False,
    loftq_config=None,
)

# Load and format dataset
train_path = "../data/train.jsonl"
val_path = "../data/val.jsonl"

def load_data(path):
    with open(path) as f:
        return [json.loads(line) for line in f if line.strip()]

from datasets import Dataset
train_dataset = Dataset.from_list(load_data(train_path))
val_dataset = Dataset.from_list(load_data(val_path))

# Training config
sft_config = SFTConfig(
    output_dir="outputs_inst",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    warmup_steps=50,
    num_train_epochs=3,
    max_steps=1000,
    learning_rate=5e-5,
    logging_dir="outputs_inst/logs",
    logging_strategy="steps",
    logging_steps=10,
    eval_steps=50,
    eval_strategy="steps",
    save_strategy="steps",
    save_steps=100,
    fp16=not is_bfloat16_supported(),
    bf16=is_bfloat16_supported(),
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="cosine",
    seed=3407,
    report_to=["none"],
    dataloader_num_workers=0,
    remove_unused_columns=False,
)

# Logging callback
class PrintLogsCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            print(f"Step {state.global_step}: {logs}")

# Trainer setup
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    dataset_text_field=None,  # Handled by custom collator
    max_seq_length=MAX_SEQ_LENGTH,
    dataset_num_proc=1,
    packing=False,
    args=sft_config,
    data_collator=PromptCompletionCollator(tokenizer)
)

trainer.add_callback(PrintLogsCallback())

# Train
trainer.train()
trainer.save_model("outputs_inst/final_model")

# Test inference
FastLanguageModel.for_inference(model)
test_prompt = "<s>[INST] Convert HL7 message to FHIR JSON for a patient with DOB 1985-06-01 and gender female [/INST]"
tok = tokenizer(test_prompt, return_tensors="pt").to("cuda")
out = model.generate(
    input_ids=tok.input_ids,
    attention_mask=tok.attention_mask,
    max_new_tokens=128,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.15,
    no_repeat_ngram_size=3,
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
print(tokenizer.decode(out[0][tok.input_ids.shape[1]:], skip_special_tokens=True))
