# Simple Llama Fine-tuning with Unsloth
# A clean, straightforward notebook for supervised fine-tuning

# ============================================================================
# CELL 1: Install Dependencies
# ============================================================================

# Install required packages
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps trl peft accelerate bitsandbytes

# ============================================================================
# CELL 2: Import Libraries
# ============================================================================

import torch
from unsloth import FastLanguageModel
from datasets import Dataset
from trl import SFTTrainer
from transformers import TrainingArguments
import json

# Check GPU
print(f"GPU available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU name: {torch.cuda.get_device_name()}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# ============================================================================
# CELL 3: Configuration (Easy to modify)
# ============================================================================

# Model settings
MODEL_NAME = "unsloth/llama-2-7b-chat-bnb-4bit"  # Change this for different models
MAX_SEQ_LENGTH = 2048

# Training settings
BATCH_SIZE = 2
LEARNING_RATE = 2e-4
EPOCHS = 1
OUTPUT_DIR = "./llama-finetuned"

# LoRA settings (keep these for efficiency)
LORA_R = 16
LORA_ALPHA = 16

print("Configuration loaded!")

# ============================================================================
# CELL 4: Load Model
# ============================================================================

# Load model and tokenizer with Unsloth optimizations
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,  # Auto-detect
    load_in_4bit=True,  # Use 4-bit for memory efficiency
)

print("‚úÖ Model loaded successfully!")

# ============================================================================
# CELL 5: Setup LoRA
# ============================================================================

# Add LoRA adapters for efficient fine-tuning
model = FastLanguageModel.get_peft_model(
    model,
    r=LORA_R,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "up_proj", "down_proj"],
    lora_alpha=LORA_ALPHA,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
)

# Show trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())

print(f"‚úÖ LoRA setup complete!")
print(f"Trainable parameters: {trainable_params:,}")
print(f"Total parameters: {total_params:,}")
print(f"Trainable %: {100 * trainable_params / total_params:.2f}%")

# ============================================================================
# CELL 6: Prepare Your Data
# ============================================================================

# Option 1: Use sample data (replace with your own)
sample_data = [
    {
        "text": "### Human: What is machine learning?\n### Assistant: Machine learning is a branch of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task."
    },
    {
        "text": "### Human: Explain neural networks simply.\n### Assistant: Neural networks are computing systems inspired by the human brain. They consist of interconnected nodes (neurons) that process information and learn patterns from data."
    },
    {
        "text": "### Human: What is Python used for?\n### Assistant: Python is a versatile programming language used for web development, data science, machine learning, automation, and many other applications due to its simplicity and extensive libraries."
    },
    {
        "text": "### Human: How do I start learning programming?\n### Assistant: Start with a beginner-friendly language like Python, practice with small projects, use online tutorials, and gradually build more complex applications as you gain confidence."
    }
]

# Option 2: Load from file (uncomment and modify as needed)
# with open('your_data.json', 'r') as f:
#     sample_data = json.load(f)

# Option 3: Load from HuggingFace dataset (uncomment and modify)
# from datasets import load_dataset
# dataset = load_dataset("your-dataset-name")
# sample_data = dataset["train"]

# Create dataset
dataset = Dataset.from_list(sample_data)
print(f"‚úÖ Dataset loaded with {len(dataset)} examples")
print(f"Sample: {dataset[0]['text'][:100]}...")

# ============================================================================
# CELL 7: Setup Training
# ============================================================================

# Training arguments
training_args = TrainingArguments(
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    warmup_steps=5,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    logging_steps=1,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    seed=42,
    output_dir=OUTPUT_DIR,
    save_strategy="epoch",
    report_to=None,  # Disable wandb/tensorboard for simplicity
)

# Create trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    dataset_num_proc=2,
    packing=False,
    args=training_args,
)

print("‚úÖ Trainer setup complete!")

# ============================================================================
# CELL 8: Start Training
# ============================================================================

print("üöÄ Starting training...")
print("This may take a while depending on your data size and GPU...")

# Show memory before training
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU memory before training: {start_gpu_memory} GB / {max_memory} GB")

# Train the model
trainer_stats = trainer.train()

# Show memory after training
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_training = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)

print(f"‚úÖ Training completed!")
print(f"Final GPU memory: {used_memory} GB / {max_memory} GB ({used_percentage}%)")
print(f"Memory used for training: {used_memory_for_training} GB")
print(f"Training loss: {trainer_stats.training_loss:.4f}")

# ============================================================================
# CELL 9: Save Model
# ============================================================================

# Save the fine-tuned model
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"‚úÖ Model saved to {OUTPUT_DIR}")

# Optional: Save to HuggingFace Hub
# model.push_to_hub("your-username/your-model-name", token="your-token")
# tokenizer.push_to_hub("your-username/your-model-name", token="your-token")

# ============================================================================
# CELL 10: Test Your Model
# ============================================================================

# Enable fast inference
FastLanguageModel.for_inference(model)

# Test function
def chat_with_model(question, max_length=200):
    prompt = f"### Human: {question}\n### Assistant:"
    
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            use_cache=True,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Extract just the assistant's response
    assistant_response = response.split("### Assistant:")[-1].strip()
    return assistant_response

# Test with some questions
test_questions = [
    "What is artificial intelligence?",
    "How do computers work?",
    "Explain the concept of algorithms",
]

print("üß™ Testing your fine-tuned model:\n")
for question in test_questions:
    print(f"Q: {question}")
    answer = chat_with_model(question)
    print(f"A: {answer}\n")
    print("-" * 50)

# ============================================================================
# CELL 11: Export for Production (Optional)
# ============================================================================

# Option 1: Export to GGUF format for llama.cpp
# model.save_pretrained_gguf(OUTPUT_DIR, tokenizer, quantization_method="q4_k_m")

# Option 2: Export to merged format (combines base model + LoRA)
# model.save_pretrained_merged(OUTPUT_DIR, tokenizer, save_method="merged_16bit")

print("‚úÖ All done! Your model is ready to use.")
print(f"Model saved in: {OUTPUT_DIR}")
print("\nTo use your model later:")
print("1. Load with: FastLanguageModel.from_pretrained('path/to/model')")
print("2. Use the chat_with_model() function above")
print("3. Or integrate into your applications!")

# ============================================================================
# CELL 12: Quick Usage Summary
# ============================================================================

print("""
üìã QUICK SUMMARY:

1. üîß Modified configuration in Cell 3
2. üìä Loaded model and setup LoRA in Cells 4-5  
3. üìÅ Prepared your data in Cell 6
4. üèÉ Trained the model in Cells 7-8
5. üíæ Saved the model in Cell 9
6. üß™ Tested the model in Cell 10

üéØ NEXT STEPS:
- Replace sample data with your own dataset
- Adjust hyperparameters if needed
- Test with more examples
- Deploy your model!

üí° TIPS:
- Increase EPOCHS for better results
- Add more training data for better performance  
- Monitor GPU memory usage
- Use validation data to prevent overfitting
""")