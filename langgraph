import json
import logging
from typing import Dict, List, Any, Optional, TypedDict, Annotated
from dataclasses import dataclass
from enum import Enum

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.sqlite import SqliteSaver

# Your existing imports
from llm.openai_client import get_azure_client, chat_completion
from prompts.code_generation_prompt import generate_prompt_code
from prompts.rule_validation_prompt import generate_prompt_rule
from code_generation import rule_evaluate_batch
from utils.utils import load_data, get_calculated_value

class AgentState(TypedDict):
    """State for the LangGraph agent"""
    # Input data
    target_field: str
    client_name: str
    tdd_document: str
    source_schema: Dict[str, Any]
    cbms_data: Dict[str, Any]
    evaluation_results: Optional[Dict[str, Any]]
    
    # Processing state
    current_step: str
    iteration_count: int
    max_iterations: int
    
    # Generated outputs
    tdd_analysis: Optional[Dict[str, Any]]
    business_rules: Optional[Dict[str, Any]]
    generated_code: Optional[str]
    validation_results: Optional[Dict[str, Any]]
    improvement_suggestions: Optional[List[Dict[str, Any]]]
    
    # Messages and logs
    messages: Annotated[List[Dict[str, str]], add_messages]
    error_log: List[str]
    
    # Final outputs
    final_rules: Optional[Dict[str, Any]]
    final_code: Optional[str]
    confidence_score: Optional[float]

class ProcessingStep(Enum):
    TDD_ANALYSIS = "tdd_analysis"
    RULE_GENERATION = "rule_generation"
    CODE_GENERATION = "code_generation"
    VALIDATION = "validation"
    IMPROVEMENT = "improvement"
    FINALIZATION = "finalization"

@dataclass
class AgentConfig:
    """Configuration for the LangGraph agent"""
    max_iterations: int = 3
    confidence_threshold: float = 0.85
    enable_auto_improvement: bool = True
    api_version: str = "2024-02-15-preview"
    max_tokens: int = 10000

class RuleGenerationAgent:
    """LangGraph-based agent for rule generation and validation"""
    
    def __init__(self, config: AgentConfig = None):
        self.config = config or AgentConfig()
        self.client = get_azure_client()
        self.logger = logging.getLogger(__name__)
        
        # Build the graph
        self.graph = self._build_graph()
        
        # Add checkpointing for persistence
        self.memory = SqliteSaver.from_conn_string(":memory:")
        self.app = self.graph.compile(checkpointer=self.memory)
    
    def _build_graph(self) -> StateGraph:
        """Build the LangGraph workflow"""
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("tdd_analyzer", self._analyze_tdd)
        workflow.add_node("rule_generator", self._generate_rules)
        workflow.add_node("code_generator", self._generate_code)
        workflow.add_node("validator", self._validate_results)
        workflow.add_node("improver", self._improve_solution)
        workflow.add_node("finalizer", self._finalize_results)
        
        # Define the flow
        workflow.set_entry_point("tdd_analyzer")
        
        # TDD Analysis → Rule Generation
        workflow.add_edge("tdd_analyzer", "rule_generator")
        
        # Rule Generation → Code Generation
        workflow.add_edge("rule_generator", "code_generator")
        
        # Code Generation → Validation
        workflow.add_edge("code_generator", "validator")
        
        # Conditional edges from validator
        workflow.add_conditional_edges(
            "validator",
            self._should_improve,
            {
                "improve": "improver",
                "finalize": "finalizer"
            }
        )
        
        # Improvement → Rule Generation (loop back)
        workflow.add_conditional_edges(
            "improver",
            self._should_continue_iteration,
            {
                "continue": "rule_generator",
                "finalize": "finalizer"
            }
        )
        
        # Finalizer → END
        workflow.add_edge("finalizer", END)
        
        return workflow
    
    def _analyze_tdd(self, state: AgentState) -> AgentState:
        """Analyze TDD document for relevant rules and requirements"""
        self.logger.info(f"[TDD_ANALYSIS] Analyzing TDD for field: {state['target_field']}")
        
        tdd_analysis_prompt = f"""
        You are an expert in analyzing Technical Design Documents (TDD) for data transformation requirements.
        
        **Task**: Analyze the TDD document for field: {state['target_field']}
        
        **TDD Document**:
        {state['tdd_document']}
        
        **Instructions**:
        1. Identify all sections relevant to {state['target_field']}
        2. Extract business rules and transformation logic
        3. Note dependencies on other fields or rules
        4. Identify validation requirements
        5. List any special conditions or edge cases
        
        **Output Format** (JSON):
        {{
            "relevant_sections": ["section_numbers"],
            "business_requirements": "detailed_description",
            "dependencies": ["field1", "field2"],
            "validation_rules": ["rule1", "rule2"],
            "edge_cases": ["case1", "case2"],
            "complexity_assessment": "simple|moderate|complex"
        }}
        """
        
        try:
            response = chat_completion(
                client=self.client,
                deployment=self.config.api_version,
                user_question=tdd_analysis_prompt,
                max_tokens=self.config.max_tokens
            )
            
            tdd_analysis = json.loads(response.replace("```json", "").replace("```", ""))
            
            state["tdd_analysis"] = tdd_analysis
            state["current_step"] = ProcessingStep.TDD_ANALYSIS.value
            state["messages"].append({
                "role": "system", 
                "content": f"TDD analysis completed for {state['target_field']}"
            })
            
        except Exception as e:
            error_msg = f"TDD analysis failed: {str(e)}"
            state["error_log"].append(error_msg)
            self.logger.error(error_msg)
        
        return state
    
    def _generate_rules(self, state: AgentState) -> AgentState:
        """Generate business rules based on TDD analysis"""
        self.logger.info(f"[RULE_GENERATION] Generating rules for: {state['target_field']}")
        
        # Use your existing rule generation prompt
        rule_prompt = generate_prompt_rule(state['target_field'])
        
        # Enhance with TDD analysis context
        enhanced_prompt = f"""
        {rule_prompt}
        
        **TDD Analysis Context**:
        {json.dumps(state.get('tdd_analysis', {}), indent=2)}
        
        **Previous Iteration Feedback** (if any):
        {json.dumps(state.get('improvement_suggestions', []), indent=2)}
        
        **Generate comprehensive business rules following Map2Common standards.**
        """
        
        try:
            response = chat_completion(
                client=self.client,
                deployment=self.config.api_version,
                user_question=enhanced_prompt,
                max_tokens=self.config.max_tokens
            )
            
            # Parse the rules (assuming JSON format)
            business_rules = json.loads(response.replace("```json", "").replace("```", ""))
            
            state["business_rules"] = business_rules
            state["current_step"] = ProcessingStep.RULE_GENERATION.value
            state["messages"].append({
                "role": "system",
                "content": f"Business rules generated for {state['target_field']}"
            })
            
        except Exception as e:
            error_msg = f"Rule generation failed: {str(e)}"
            state["error_log"].append(error_msg)
            self.logger.error(error_msg)
        
        return state
    
    def _generate_code(self, state: AgentState) -> AgentState:
        """Generate Python transformation code"""
        self.logger.info(f"[CODE_GENERATION] Generating code for: {state['target_field']}")
        
        # Use your existing code generation prompt
        fields_used_from_cbms = state.get('cbms_data', {})
        input_cols = str(state.get('source_schema', {}).keys())
        
        code_prompt = generate_prompt_code(
            business_rule=json.dumps(state.get('business_rules', {})),
            fields_used_from_cbms=fields_used_from_cbms,
            input_cols=input_cols
        )
        
        # Add context about previous iterations
        enhanced_code_prompt = f"""
        {code_prompt}
        
        **Previous Code Issues** (if any):
        {json.dumps(state.get('improvement_suggestions', []), indent=2)}
        
        **Generate production-ready Python code with comprehensive error handling and logging.**
        """
        
        try:
            response = chat_completion(
                client=self.client,
                deployment=self.config.api_version,
                user_question=enhanced_code_prompt,
                system_prompt="You are a Python code generation expert specializing in data transformation.",
                max_tokens=self.config.max_tokens,
                prefix="TDD is: "
            )
            
            # Extract Python code from response
            code_json = json.loads(response.replace("```json", "").replace("```", ""))
            generated_code = code_json.get("python_code", "")
            
            state["generated_code"] = generated_code
            state["current_step"] = ProcessingStep.CODE_GENERATION.value
            state["messages"].append({
                "role": "system",
                "content": f"Python code generated for {state['target_field']}"
            })
            
        except Exception as e:
            error_msg = f"Code generation failed: {str(e)}"
            state["error_log"].append(error_msg)
            self.logger.error(error_msg)
        
        return state
    
    def _validate_results(self, state: AgentState) -> AgentState:
        """Validate generated code using rule_evaluate_batch"""
        self.logger.info(f"[VALIDATION] Validating results for: {state['target_field']}")
        
        try:
            # Save generated code to file (temporary)
            code_file = f"./transformation_codes/{state['target_field']}.py"
            with open(code_file, 'w') as f:
                f.write(state.get('generated_code', ''))
            
            # Load your data using existing utilities
            inmemory_data = load_data(
                client_name=state['client_name'],
                client_to_id=state.get('client_to_id', {}),
                txt_files=state.get('txt_files', []),
                client_to_json=state.get('client_to_json', {}),
                client_to_conformity_csv=state.get('client_to_conformity_csv', {})
            )
            
            # Run evaluation
            all_eval_dfs, all_error_dfs = rule_evaluate_batch(
                target_fields=[state['target_field']],
                inmemory_data=inmemory_data,
                n_random=1,  # Use small sample for validation
                parquet_dir=state.get('parquet_dir', './data'),
                error_log_dir=state.get('error_log_dir', './errors'),
                output_dir=state.get('output_dir', './outputs'),
                table_cache=state.get('table_cache', {})
            )
            
            # Calculate accuracy
            validation_results = self._calculate_validation_metrics(
                all_eval_dfs, all_error_dfs, state['target_field']
            )
            
            state["validation_results"] = validation_results
            state["current_step"] = ProcessingStep.VALIDATION.value
            state["messages"].append({
                "role": "system",
                "content": f"Validation completed. Accuracy: {validation_results.get('accuracy', 0):.2%}"
            })
            
        except Exception as e:
            error_msg = f"Validation failed: {str(e)}"
            state["error_log"].append(error_msg)
            self.logger.error(error_msg)
            state["validation_results"] = {"accuracy": 0.0, "error": error_msg}
        
        return state
    
    def _improve_solution(self, state: AgentState) -> AgentState:
        """Analyze validation results and suggest improvements"""
        self.logger.info(f"[IMPROVEMENT] Analyzing issues for: {state['target_field']}")
        
        validation_results = state.get('validation_results', {})
        
        improvement_prompt = f"""
        Analyze the validation results and provide specific improvement suggestions.
        
        **Target Field**: {state['target_field']}
        **Current Accuracy**: {validation_results.get('accuracy', 0):.2%}
        **Validation Results**: {json.dumps(validation_results, indent=2)}
        **Current Business Rules**: {json.dumps(state.get('business_rules', {}), indent=2)}
        **Generated Code**: {state.get('generated_code', '')}
        
        **Provide specific improvements in JSON format**:
        {{
            "rule_improvements": [
                {{
                    "issue": "description_of_issue",
                    "solution": "specific_fix",
                    "priority": "high|medium|low"
                }}
            ],
            "code_improvements": [
                {{
                    "issue": "description_of_issue", 
                    "solution": "specific_fix",
                    "priority": "high|medium|low"
                }}
            ],
            "overall_recommendations": ["recommendation1", "recommendation2"]
        }}
        """
        
        try:
            response = chat_completion(
                client=self.client,
                deployment=self.config.api_version,
                user_question=improvement_prompt,
                max_tokens=self.config.max_tokens
            )
            
            improvements = json.loads(response.replace("```json", "").replace("```", ""))
            
            state["improvement_suggestions"] = improvements
            state["iteration_count"] = state.get("iteration_count", 0) + 1
            state["current_step"] = ProcessingStep.IMPROVEMENT.value
            
        except Exception as e:
            error_msg = f"Improvement analysis failed: {str(e)}"
            state["error_log"].append(error_msg)
            self.logger.error(error_msg)
        
        return state
    
    def _finalize_results(self, state: AgentState) -> AgentState:
        """Finalize and package the results"""
        self.logger.info(f"[FINALIZATION] Finalizing results for: {state['target_field']}")
        
        # Calculate final confidence score
        accuracy = state.get('validation_results', {}).get('accuracy', 0)
        iteration_penalty = max(0, (state.get('iteration_count', 0) - 1) * 0.1)
        confidence_score = max(0, accuracy - iteration_penalty)
        
        state["final_rules"] = state.get('business_rules', {})
        state["final_code"] = state.get('generated_code', '')
        state["confidence_score"] = confidence_score
        state["current_step"] = ProcessingStep.FINALIZATION.value
        
        state["messages"].append({
            "role": "system",
            "content": f"Processing completed. Final confidence: {confidence_score:.2%}"
        })
        
        return state
    
    def _should_improve(self, state: AgentState) -> str:
        """Decide whether to improve or finalize"""
        validation_results = state.get('validation_results', {})
        accuracy = validation_results.get('accuracy', 0)
        iteration_count = state.get('iteration_count', 0)
        
        # Improve if accuracy is below threshold and haven't exceeded max iterations
        if (accuracy < self.config.confidence_threshold and 
            iteration_count < self.config.max_iterations and
            self.config.enable_auto_improvement):
            return "improve"
        else:
            return "finalize"
    
    def _should_continue_iteration(self, state: AgentState) -> str:
        """Decide whether to continue iterating or finalize"""
        iteration_count = state.get('iteration_count', 0)
        
        if iteration_count < self.config.max_iterations:
            return "continue"
        else:
            return "finalize"
    
    def _calculate_validation_metrics(self, all_eval_dfs: Dict, all_error_dfs: Dict, target_field: str) -> Dict[str, Any]:
        """Calculate validation metrics from evaluation results"""
        try:
            eval_df = all_eval_dfs.get(target_field)
            error_df = all_error_dfs.get(target_field)
            
            if eval_df is None or eval_df.empty:
                return {"accuracy": 0.0, "total_records": 0, "error": "No evaluation data"}
            
            # Calculate accuracy (assuming columns: rule_agent_value, eagle_value)
            total_records = len(eval_df)
            if total_records == 0:
                return {"accuracy": 0.0, "total_records": 0}
            
            # Compare predicted vs actual values
            correct_predictions = (eval_df['rule_agent_value'] == eval_df['eagle_value']).sum()
            accuracy = correct_predictions / total_records
            
            error_count = len(error_df) if error_df is not None else 0
            
            return {
                "accuracy": accuracy,
                "total_records": total_records,
                "correct_predictions": correct_predictions,
                "error_count": error_count,
                "success_rate": (total_records - error_count) / total_records if total_records > 0 else 0
            }
            
        except Exception as e:
            return {"accuracy": 0.0, "error": str(e)}
    
    def run(self, 
            target_field: str,
            client_name: str, 
            tdd_document: str,
            source_schema: Dict[str, Any],
            cbms_data: Dict[str, Any],
            **kwargs) -> Dict[str, Any]:
        """Run the complete agent workflow"""
        
        # Initialize state
        initial_state = AgentState(
            target_field=target_field,
            client_name=client_name,
            tdd_document=tdd_document,
            source_schema=source_schema,
            cbms_data=cbms_data,
            evaluation_results=kwargs.get('evaluation_results'),
            current_step="initialized",
            iteration_count=0,
            max_iterations=self.config.max_iterations,
            tdd_analysis=None,
            business_rules=None,
            generated_code=None,
            validation_results=None,
            improvement_suggestions=None,
            messages=[],
            error_log=[],
            final_rules=None,
            final_code=None,
            confidence_score=None
        )
        
        # Add any additional parameters
        for key, value in kwargs.items():
            if key not in initial_state:
                initial_state[key] = value
        
        # Run the workflow
        config = {"configurable": {"thread_id": f"{client_name}_{target_field}"}}
        
        try:
            final_state = self.app.invoke(initial_state, config)
            
            return {
                "success": True,
                "target_field": target_field,
                "final_rules": final_state.get("final_rules"),
                "final_code": final_state.get("final_code"),
                "confidence_score": final_state.get("confidence_score"),
                "validation_results": final_state.get("validation_results"),
                "iteration_count": final_state.get("iteration_count"),
                "messages": final_state.get("messages"),
                "errors": final_state.get("error_log")
            }
            
        except Exception as e:
            self.logger.error(f"Agent execution failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "target_field": target_field
            }

# Usage example and helper functions
class AgentRunner:
    """Helper class to run the agent with your existing data"""
    
    def __init__(self, agent_config: AgentConfig = None):
        self.agent = RuleGenerationAgent(agent_config)
    
    def run_for_target_field(self,
                           target_field: str,
                           client_name: str = "Oscar HealthFirst",
                           tdd_document_path: str = None,
                           **kwargs) -> Dict[str, Any]:
        """Run agent for a specific target field with your existing setup"""
        
        # Load TDD document
        if tdd_document_path:
            with open(tdd_document_path, 'r') as f:
                tdd_document = f.read()
        else:
            tdd_document = kwargs.get('tdd_document', '')
        
        # Load your existing data structures
        client_to_id = kwargs.get('client_to_id', {
            "Oscar HealthFirst": {
                "client_id": "33019",
                "file_id": [162014]
            }
        })
        
        client_to_conformity_csv = kwargs.get('client_to_conformity_csv', {})
        
        # Get source schema from your conformity data
        source_schema = {}
        if client_to_conformity_csv and client_name in client_to_conformity_csv:
            # Load schema from conformity CSV
            pass  # Implement based on your data structure
        
        # Load CBMS data
        cbms_data = kwargs.get('cbms_data', {})
        
        # Run the agent
        result = self.agent.run(
            target_field=target_field,
            client_name=client_name,
            tdd_document=tdd_document,
            source_schema=source_schema,
            cbms_data=cbms_data,
            client_to_id=client_to_id,
            client_to_conformity_csv=client_to_conformity_csv,
            **kwargs
        )
        
        return result

# Example usage:
if __name__ == "__main__":
    # Configure the agent
    config = AgentConfig(
        max_iterations=3,
        confidence_threshold=0.85,
        enable_auto_improvement=True
    )
    
    # Create and run the agent
    runner = AgentRunner(config)
    
    result = runner.run_for_target_field(
        target_field="grgr_id",  # Your target field
        client_name="Oscar HealthFirst",
        tdd_document_path="path/to/your/tdd.pdf",  # Path to your TDD
        # Add other parameters as needed
    )
    
    print(f"Success: {result['success']}")
    if result['success']:
        print(f"Confidence: {result['confidence_score']:.2%}")
        print(f"Iterations: {result['iteration_count']}")
        print("Generated Rules:", result['final_rules'])
        print("Generated Code:", result['final_code'])
    else:
        print(f"Error: {result['error']}")
